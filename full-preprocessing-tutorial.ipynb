{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rubens/full-preprocessing-tutorial?scriptVersionId=97036024\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"## Introduction\n\nWorking with these files can be a challenge, especially given their heterogeneous nature. Some preprocessing is required before they are ready for consumption by your CNN.\n\nFortunately, I participated in the LUNA16 competition as part of a university course on computer aided diagnosis, so I have some experience working with these files. At this moment we top the leaderboard there :)\n\n**This tutorial aims to provide a comprehensive overview of useful steps to take before the data hits your ConvNet/other ML method.**\n\nWhat we will cover:  \n\n* **Loading the DICOM files**, and adding missing metadata  \n* **Converting the pixel values to *Hounsfield Units (HU)***, and what tissue these unit values correspond to\n* **Resampling** to an isomorphic resolution to remove variance in scanner resolution.\n* **3D plotting**, visualization is very useful to see what we are doing.\n* **Lung segmentation**\n* **Normalization** that makes sense.\n* **Zero centering** the scans.\n\n\n---\n\nBefore we start, let's import some packages and determine the available patients.","metadata":{"_cell_guid":"a07f5103-d036-968f-40d7-57649bbb4864"}},{"cell_type":"code","source":"%matplotlib inline\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dicom\nimport os\nimport scipy.ndimage\nimport matplotlib.pyplot as plt\n\nfrom skimage import measure\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\n# Some constants \nINPUT_FOLDER = '../input/sample_images/'\npatients = os.listdir(INPUT_FOLDER)\npatients.sort()","metadata":{"_cell_guid":"1d12eab6-3340-fa57-84a1-91fe13886996"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the files\nDicom is the de-facto file standard in medical imaging. This is my first time working with it, but it seems to be fairly straight-forward.  These files contain a lot of metadata (such as the pixel size, so how long one pixel is in every dimension in the real world). \n\nThis pixel size/coarseness of the scan differs from scan to scan (e.g. the distance between slices may differ), which can hurt performance of CNN approaches. We can deal with this by isomorphic resampling, which we will do later.\n\nBelow is code to load a scan, which consists of multiple slices, which we simply save in a Python list. Every folder in the dataset is one scan (so one patient). One metadata field is missing, the pixel size in the Z direction, which is the slice thickness. Fortunately we can infer this, and we add this to the metadata.\n","metadata":{"_cell_guid":"56147dd5-3127-8844-644d-0b1565c9dac3"}},{"cell_type":"code","source":"# Load the scans in given folder path\ndef load_scan(path):\n    slices = [dicom.read_file(path + '/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.InstanceNumber))\n    \n    slice_thickness = slices[0].SliceLocation - slices[1].SliceLocation\n    for s in slices:\n        s.SliceThickness = slice_thickness\n        \n    return slices","metadata":{"_cell_guid":"4f50e5b1-c1e8-14a8-591b-eb466e5adc0d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The unit of measurement in CT scans is the **Hounsfield Unit (HU)**, which is a measure of radiodensity. CT scanners are carefully calibrated to accurately measure this.  From Wikipedia:\n\n![HU examples][1]\n\nBy default however, the returned values are not in this unit. Let's fix this.\n\nSome scanners have cylindrical scanning bounds, but the output image is square. The pixels that fall outside of these bounds get the fixed value -2000. The first step is setting these values to 0, which currently corresponds to air. Next, let's go back to HU units, by adding the intercept (which is conveniently stored in the metadata of the scans!).\n\n  [1]: http://i.imgur.com/4rlyReh.png","metadata":{"_cell_guid":"79a91d86-1009-ac6d-14d4-a1c117f3dd64"}},{"cell_type":"code","source":"def get_pixels_hu(scans):\n    image = np.stack([s.pixel_array for s in scans])\n    \n    # Set outside-of-scan pixels to 0\n    # The intercept is usually -1024, so air is approximately 0\n    image[image == -2000] = 0\n    \n    # Convert to Hounsfield units (HU)\n      \n    intercept = scans[0].RescaleIntercept\n    image += int(intercept)\n    \n    return np.array(image)","metadata":{"_cell_guid":"e60f402c-0cd9-f0d2-0c3d-3e7af98f9845"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at one of the patients.","metadata":{"_cell_guid":"9a64cb22-2b5d-425c-7ced-0e977ca04fa7"}},{"cell_type":"code","source":"first_patient = load_scan(INPUT_FOLDER + patients[0])\nfirst_patient_pixels = get_pixels_hu(first_patient)\nplt.hist(first_patient_pixels.flatten(), bins=80, color='c')\nplt.xlabel(\"Hounsfield Units (HU)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\n# Show some slice in the middle\nplt.imshow(first_patient_pixels[80], cmap=plt.cm.gray)\nplt.show()","metadata":{"_cell_guid":"b0012bd3-50f6-278b-de58-a2a39361f1bb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the table from Wikipedia and this histogram, we can clearly see which pixels are air and which are tissue. We will use this for lung segmentation in a bit :)\n\n\n----------\n","metadata":{"_cell_guid":"894982a5-844c-ebb2-a48c-62a057cf5ec9"}},{"cell_type":"markdown","source":"# Resampling\nA scan may have a pixel spacing of `[2.5, 0.5, 0.5]`, which means that the distance between slices is `2.5` millimeters. For a different scan this may be `[1.5, 0.725, 0.725]`, this can be problematic for automatic analysis (e.g. using ConvNets)! \n\nA common method of dealing with this is resampling the full dataset to a certain isotropic resolution. If we choose to resample everything to 1mm*1mm*1mm pixels we can use 3D convnets without worrying about learning zoom/slice thickness invariance. \n\nWhilst this may seem like a very simple step, it has quite some edge cases due to rounding. Also, it takes quite a while.\n\nBelow code worked well for us (and deals with the edge cases):","metadata":{"_cell_guid":"ae70ecba-82a6-099b-ac6a-c2f69c055929"}},{"cell_type":"code","source":"def resample(image, scan, new_spacing=[1,1,1]):\n    # Determine current pixel spacing\n    spacing = map(float, ([scan[0].SliceThickness] + scan[0].PixelSpacing))\n    spacing = np.array(list(spacing))\n\n    resize_factor = spacing / new_spacing\n    new_real_shape = image.shape * resize_factor\n    new_shape = np.round(new_real_shape)\n    real_resize_factor = new_shape / image.shape\n    new_spacing = spacing / real_resize_factor\n    \n    image = scipy.ndimage.interpolation.zoom(image, real_resize_factor)\n    \n    return image, new_spacing","metadata":{"_cell_guid":"59c11a48-c6d0-8522-f5cd-b03d8ee812f6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Please note that when you apply this, to save the new spacing! Due to rounding this may be slightly off from the desired spacing (above script picks the best possible spacing with rounding).\n\nLet's resample our patient's pixels to an isomorphic resolution of 1 by 1 by 1 mm.","metadata":{"_cell_guid":"04b36366-5dd1-90b1-ad19-065127fe4720"}},{"cell_type":"code","source":"pix_resampled, spacing = resample(first_patient_pixels, first_patient, [1,1,1])\nprint(\"Shape before resampling\\t\", first_patient_pixels.shape)\nprint(\"Shape after resampling\\t\", pix_resampled.shape)","metadata":{"_cell_guid":"53acb2e7-1edc-8212-3b6d-30eb16d75025"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3D plotting the scan\nFor visualization it is useful to be able to show a 3D image of the scan. Unfortunately, the packages available in this Kaggle docker image is very limited in this sense, so we will use marching cubes to create an approximate mesh for our 3D object, and plot this with matplotlib. Quite slow and ugly, but the best we can do.\n","metadata":{"_cell_guid":"405beab9-b95f-9ea6-6f51-235943c1d5dc"}},{"cell_type":"code","source":"def plot_3d(image, threshold=-300):\n    \n    # Position the scan upright, \n    # so the head of the patient would be at the top facing the camera\n    p = image.transpose(2,1,0)\n    p = p[:,:,::-1]\n    \n    verts, faces = measure.marching_cubes(p, threshold)\n\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Fancy indexing: `verts[faces]` to generate a collection of triangles\n    mesh = Poly3DCollection(verts[faces], alpha=0.1)\n    face_color = [0.5, 0.5, 1]\n    mesh.set_facecolor(face_color)\n    ax.add_collection3d(mesh)\n\n    ax.set_xlabel(\"x-axis\")\n    ax.set_ylabel(\"y-axis\")\n    ax.set_zlabel(\"z-axis\")\n\n    ax.set_xlim(0, p.shape[0])  # a = 6 (times two for 2nd ellipsoid)\n    ax.set_ylim(0, p.shape[1])  # b = 10\n    ax.set_zlim(0, p.shape[2])  # c = 16\n\n    plt.show()","metadata":{"_cell_guid":"05a89e78-9a12-c019-8693-efb4030172bd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our plot function takes a threshold argument which we can use to plot certain structures, such as all tissue or only the bones. 400 is a good threshold for showing the bones only (see Hounsfield unit table above). Let's do this!","metadata":{"_cell_guid":"60f79f87-44cc-620e-259c-d18d2e085bd6"}},{"cell_type":"code","source":"plot_3d(pix_resampled, 400)","metadata":{"_cell_guid":"8373d7cb-c307-b8a3-c9a4-6d3daec2cee5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Spooky!\n\n# Lung segmentation\nIn order to reduce the problem space, we can segment the lungs (and usually some tissue around it). The method that me and my student colleagues developed was quite effective. It consists of a series of applications of region growing and morphological operations.\n\n**This section is not finished yet, to do!**","metadata":{"_cell_guid":"375b567c-90eb-dee2-6fc6-e6463833640e"}},{"cell_type":"markdown","source":"# Normalization\nOur values currently range from -1024 to around 2000. Anything above 400 is not interesting to us, as these are simply bones with different radiodensity.  A commonly used set of thresholds in the LUNA16 competition to normalize between are -1000 and 400. Here's some code you can use:","metadata":{"_cell_guid":"756185c7-1833-66b8-572d-30d480533ad0"}},{"cell_type":"code","source":"MIN_BOUND = -1000.0\nMAX_BOUND = 400.0\n    \ndef normalize(image):\n    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n    image[image>1] = 1.\n    image[image<0] = 0.\n    return image","metadata":{"_cell_guid":"b65e2226-a7a5-777d-5e4c-2e87e2f708a9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Zero centering\n\nAs a final preprocessing step, it is advisory to zero center your data so that your mean value is 0. To do this you simply subtract the mean pixel value from all pixels. \n\nTo determine this mean you simply average all images in the whole dataset.  If that sounds like a lot of work, we found this to be around 0.25 in the LUNA16 competition. \n\n**Warning: Do not zero center with the mean per image (like is done in some kernels on here). The CT scanners are calibrated to return accurate HU measurements. There is no such thing as an image with lower contrast or brightness like in normal pictures.**","metadata":{"_cell_guid":"f2f3eacf-3a41-a36d-ecb5-9354d926e6f4"}},{"cell_type":"code","source":"PIXEL_MEAN = 0.25\n\ndef zero_center(image):\n    image = image - PIXEL_MEAN\n    return image","metadata":{"_cell_guid":"3570abfa-b13a-bd5b-06f3-9fd9a4329b1d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# What's next? \n\nWith these steps your images are ready for consumption by your CNN or other ML method :). You can do all these steps offline (one time and save the result), and I would advise you to do so and let it run overnight as it may take a long time. \n\n**Tip:** To save storage space, don't do normalization and zero centering beforehand, but do this online (during training, just after loading). If you don't do this yet, your image are int16's, which are smaller than float32s and easier to compress as well.\n\n**If this tutorial helped you at all, please upvote it and leave a comment :)**\n\n","metadata":{"_cell_guid":"17e813c3-6388-1956-47a2-24bb54376142"}}]}